# -*- coding: utf-8 -*-
"""llm_scratch_AK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-RCM-DS3aZ0pJgmx56cw5E5MJBq2EUDD

## LLM from Scratch
ok so here we are developing a character level gpt
we know, chatgpt is not character level, it uses its own tokenizer called tiktoken.
"""

# We always start with a dataset to train on. Let's download the tiny shakespeare dataset
# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

import torch
import torch.nn as nn
from torch.nn import functional as F

# hyperparameters
batch_size = 32 # how many independent sequences will we process in parallel?
block_size = 8 # what is the maximum context length for predictions?
# increasing max iters since lr is lesser
max_iters = 5000
eval_interval = 300
learning_rate = 1e-3
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
n_embd = 32
# print(device)

# read it in to inspect it
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# print("length of dataset in characters: ", len(text))

# let's look at the first 1000 characters
# print(text[:1000])

# here are all the unique characters that occur in this text
chars = sorted(list(set(text)))
vocab_size = len(chars)
# print(''.join(chars))
# print(vocab_size)

# create a mapping from characters to integers
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string

# print(encode("hii there"))
# print(decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))

# !pip install tiktoken --quiet

# Tiktoken
# import tiktoken
# enc = tiktoken.get_encoding('gpt2')
# print(enc.n_vocab)
# print()

# print(enc.encode("hii there"))
# print(enc.decode([71, 4178, 612]))

# let's now encode the entire text dataset and store it into a torch.Tensor

data = torch.tensor(encode(text), dtype=torch.long)
# print(data.shape, data.dtype)

# print(text[:100]) # the 100 characters in text we looked at earier will to the GPT look like this
# print(data[:100]) # the 100 characters in data we looked at earier will to the GPT look like this

"""## Train Test Split

ok so we wont be training the llm directly on the entire data, thats computationally way too expensive. Instead we train it on chuncks of the data
"""

# 90-10 split

leng = int(0.9*len(data))
train_data = data[:leng]
val_data = data[leng:]

# max length of chunck?
block_size = 8 # or context length

# why +1 is
# in the chunck of 9 characters we have 8 individual examples, since all characters follow each other
train_data[:block_size+1]

# input
x = train_data[:block_size]
# target, always the character after the input
y = train_data[1:block_size+1]

# the 8 examples hidden in the chunck
# for t in range(block_size):
#     context = x[:t+1]
#     target = y[t]
#     print(f"when input is {context} the target: {target}")

# we wanna process multiple chuncks at the same time

torch.manual_seed(1337)
batch_size = 4 # how many independent sequences will we process in parallel?
block_size = 8 # what is the maximum context length for predictions?

# total 32 examples

def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

@torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

xb, yb = get_batch('train')

# this is the implementation of one head of the legendary self-attention
class Head(nn.Module):
  def __init__(self, head_size):
    super().__init__()
    self.key = nn.Linear(n_embd, head_size, bias=False)
    self.query = nn.Linear(n_embd, head_size, bias=False)
    self.value = nn.Linear(n_embd, head_size, bias=False)
    # used to create the tril
    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))
    
  def forward(self, x):
    B, T, C = x.shape
    k = self.key(x)
    q = self.query(x)
    v = self.value(x)

    wei = q @ k.transpose(-2, -1)* C**-0.5
    wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))
    wei = F.softmax(wei, dim=-1)

    out = wei @ v
    return out
      

class LayerNorm1d: # (used to be BatchNorm1d)

  def __init__(self, dim, eps=1e-5, momentum=0.1):
    self.eps = eps
    self.gamma = torch.ones(dim)
    self.beta = torch.zeros(dim)

  def __call__(self, x):
    # calculate the forward pass
    xmean = x.mean(1, keepdim=True) # batch mean
    xvar = x.var(1, keepdim=True) # batch variance
    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance
    self.out = self.gamma * xhat + self.beta
    return self.out

  def parameters(self):
    return [self.gamma, self.beta]


# multiheaded attention
class MultiHeadAttention(nn.Module):
  def __init__(self, num_heads, head_size):
    super().__init__()
    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
    self.proj = nn.Linear(n_embd, n_embd)
  
  def forward(self, x):
    out = torch.cat([h(x) for h in self.heads], dim=-1)
    out = self.proj(out)
    return out


# simple feedforward layer
class FeedForward(nn.Module):
  # this is on a individual token level
  def __init__(self, n_embd):
    super().__init__()
    self.net = nn.Sequential(
                  nn.Linear(n_embd, 4*n_embd),
                  nn.ReLU(),
                  # projection layer
                  nn.Linear(4*n_embd, n_embd),
                  )
  
  def forward(self, x):
    return self.net(x)


class Block(nn.Module):
  def __init__(self, n_embd, n_head):
    super().__init__()
    head_size = n_embd // n_head
    self.sa = MultiHeadAttention(n_head, head_size)
    self.ffwd = FeedForward(n_embd)
    self.ln1 = nn.LayerNorm(n_embd)
    self.ln2 = nn.LayerNorm(n_embd)
    
  def forward(self, x):
    # residual connections 
    # layernorm before the attention and ff network, normalizes the 32 numbers
    x = x + self.sa(self.ln1(x))
    x = x + self.ffwd(self.ln2(x))
    return x
   
  
# bigram language model
torch.manual_seed(1337)

class BigramLM(nn.Module):
  def __init__(self):
    super().__init__()
    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
    self.pos_embedding_table = nn.Embedding(block_size, n_embd)
    # for single head attention
    # # self.sa_head = Head(n_embd)
    # # for multihead attention
    # self.sa_heads = MultiHeadAttention(4, n_embd//4) # ie, 4 heads of 8 dimesional self-attention
    # self.ffwd = FeedForward(n_embd)
    self.blocks = nn.Sequential(
       Block(n_embd, n_head=4),
       Block(n_embd, n_head=4),
       Block(n_embd, n_head=4),
       nn.LayerNorm(n_embd),
    )
    self.llm_head = nn.Linear(n_embd, vocab_size)

  def forward(self, idx, targets=None):
    B, T = idx.shape
    
    tok_emb = self.token_embedding_table(idx)
    pos_emb = self.pos_embedding_table(torch.arange(T, device=device))
    x = tok_emb + pos_emb
    # x = self.sa_heads(x)
    # x = self.ffwd(x)
    x = self.blocks(x)
    logits = self.llm_head(x)

    if targets==None:
      loss = None
    else:
      # block, time, channels
      # but pytorch wants b*t, c
      b, t, c = logits.shape
      # -1 should also work
      logits = logits.view(b*t, c)
      targets = targets.view(b*t)
      loss = F.cross_entropy(logits, targets)
    return logits, loss


  # get preds of current indices, then focus on last step
  def generate(self, idx, max_new_tokens):

    for _ in range(max_new_tokens):

      # to ensure crop the context acc to block size
      idx_cond = idx[:, -block_size:]

      logits, loss = self(idx_cond)
      logits = logits[:, -1, :] # make it b, c
      probs = F.softmax(logits, dim=-1) # b, c

      idx_next = torch.multinomial(probs, num_samples=1) # give us b, 1
      idx = torch.cat((idx, idx_next), dim=1)  # b, t+1

    return idx


model = BigramLM()
m = model.to(device)
# logits, loss = model(xb, yb)
# print(logits.shape)
# print(loss)

# zero is new line, so reasonable to feed newline
# idx = torch.zeros((1, 1), dtype=torch.long)
# print(decode(model.generate(idx, max_new_tokens=100)[0].tolist()))
# it sucks because we r using a totally random model and history is hot used

# big models -> 3e^-4
# this case, small model -> 1e^-3

optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):

    # every once in a while evaluate the loss on train and val sets
    if iter % eval_interval == 0:
        losses = estimate_loss()
        print(f"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# generate from the model
context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))

"""we can see some progress"""